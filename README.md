# TheFirstExhaustiveAnalysisOfEpistemology

Context: I challenge myself to committing long-term continuous work (arguably the most important life skill) at studying exhaustively the topic of Epistemology.

My criterion for exhaustiveness is: reading and summarizing the gist of ALL the articles from [the Epistemology Index](https://en.wikipedia.org/wiki/Index_of_epistemology_articles)

The choosen coverage order is alphabetical instead of hierachical.

Shall we begin:

[A Defence of Common Sense](https://en.wikipedia.org/wiki/A_Defence_of_Common_Sense):
This analytical essay summary has no value beyond truisms 0/20
  * The essay refer to a ["famous" argument of the author](https://en.wikipedia.org/wiki/Here_is_one_hand):
    * invalid proof, does not imply any definite answer about the intricate nature of 
    y. His truism about the common sense      interpretation having much higher verisimilitude than other hypothesises (matrix, dream, philosophical zombies) is.. a truism. -5/20   
    Explaining why some metaphysical hypothesises about the nature of reality are epistemologically stronger than others would have been worthwhile though.
    * todo recurse


[A posteriori/Empirical evidence](https://en.wikipedia.org/wiki/Empirical_evidence):
While understanding that all source of knowledge ( and associated meta-knowledge/verisimilitudes) eventually come down to experimental evidence, is mandatory (20/20) the article in itself (10/20) has many simplifications and ommissions so here is a rephrasing/correction:
Empirical data comes either from sensorial data (~qualias from any living being) or from Instruments. This imply that advances in either psychometrics and *metrology* (and even data storage, validation, processing and retrieval) can enable leap in mankind progress.

However, Empirical data != empirical evidence. The expected verisimilitude of a data that has been measured is a direct product of the already known/expected precision of the instrument (be it a biological sensor or a regular instrument). Moreover, contextual things can alter the precision e.g sensorial illusions, biases, and mechanical illusions. Finally, where applicable, a statistical distribution should be plotted. 
But how can we know empirically the expected precision of an instrument or effect of a contextual effect (e.g illusion) if we need to know it for knowing it ? It is of course a circular problem analoguous to many others such as Compiler Bottsraping. It is resolved by the process of induction, however while induction can be simulated on a computer, a fidel enough algorithm to simulate human induction in non-trivial cases does not yet exists to my knowledge. This illustrate how *partially* conscious we are even on the highest level cognitive functions such as attributing evidence.

Empirical data must be analysed and put in perspective with other knowledges in order to maximally extract meaning from it.
The idea that pure deduction exists is non-sensical. Deductiion and natural language in general is just manipulating empirical data with N indirections, with the added liberty that the brain allow itself to not be internally consistent, to fill missing data through (sometimes baseless) speculation and various other biases/heuristics, however with enough rigor it all reduce to empirical facts
 refer to:
  * reproducibility crisis
  * the scientific method

[A posteriori/ A priori](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori):
TODO

[A Treatise Concerning the Principles of Human Knowledge](https://en.wikipedia.org/wiki/A_Treatise_Concerning_the_Principles_of_Human_Knowledge):
I have stopped reading at the introduction, classical intellectual vomit about the nature of reality (here a world of ideas having the appearance of order (physics) thanks to.. god). Maybe that Berkeley should have read "A defense of Common Sense".. /s -10/20

Enough food for thoughts for today!



